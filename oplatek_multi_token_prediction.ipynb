{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Notes\n",
    "\n",
    "- Typical a single token prediction is trained using NextTokenPrediction (NTP) task / loss. Using Cross-Entropy (CE) loss.\n",
    "- Speculative decoding will improve the task and is cheap for inference\n",
    "- How multiple token loss will speed up training? Ignore speed-ups on inference\n",
    "\n",
    "\n",
    "## Plan\n",
    "1. Quickly scan some relevant papers. Found [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737).\n",
    "2. Recap the baseline model training in the code-base\n",
    "3. Test model export and running inference\n",
    "4. Read code & implement the auxilary heads and losses so the export is still valid\n",
    "5. Finally, experiment with the losses and hyperparameters\n",
    "\n",
    "Some thoughts:\n",
    "- CE losses for (1-default), 2, 3, 4, k NTP as the first thing to try.\n",
    "- Consistency loss between having access to -1, -2, -3 token (as standard in the masked attention) and not knowing the attention.\n",
    "\n",
    "## Problems encountered\n",
    "\n",
    "### Update the script for the smaller model\n",
    "| model | dim | n_layers | n_heads | n_kv_heads | max context length | parameters | val loss | download\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 260K | 64 | 5 | 8 | 4 | 512 | 260K | 1.297 | [stories260K](https://huggingface.co/karpathy/tinyllamas/tree/main/stories260K)\n",
    "\n",
    "### Update the code to have auxilary function \n",
    "Not optimized version simple adding `AUX_LOSS: int` function based on hyperparameter and I simply shifted the targets.\n",
    "\n",
    "## Experiments notes\n",
    "TBD and split in multiple cells\n",
    "\n",
    "## Summary\n",
    "TBD and split in multiple cells"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
