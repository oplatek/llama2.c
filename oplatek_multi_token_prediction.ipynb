{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ondrej Platek Report on Multi Token Prediction for BottleCap\n",
    "\n",
    "## Initial Notes\n",
    "\n",
    "- Typical a single token prediction is trained using NextTokenPrediction (NTP) task / loss. Using Cross-Entropy (CE) loss.\n",
    "- Speculative decoding will improve the task and is cheap for inference\n",
    "- How multiple token loss will speed up training? Ignore speed-ups on inference\n",
    "- The code is in a private repo https://github.com/oplatek/llama2.c and the experimens are logged to private https://wandb.ai/moosenet/bottlecap project. Ask me for access if interested.\n",
    "- Be sure to see also the last section `Suggestions for other approaches to speed up training`.\n",
    "\n",
    "## Plan\n",
    "1. Quickly scan some relevant papers. Found [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737).\n",
    "2. Recap the baseline model training in the code-base\n",
    "3. Test model export and running inference\n",
    "4. Read code & implement the auxilary heads and losses so the export is still valid\n",
    "5. Finally, experiment with the losses and hyperparameters\n",
    "\n",
    "Initial thoughts:\n",
    "- CE losses for (1-default), 2, 3, 4, k NTP as the first thing to try.\n",
    "- Consistency loss between having access to -1, -2, -3 token (as standard in the masked attention) and not knowing the attention.\n",
    "- Alternative and ambitios approaches to training speedup: ?Multiple-tokenizer support?, ?Diffusion based transformers?... see the end of report for more details.\n",
    "\n",
    "## Problems encountered/Progress description\n",
    "In general, most of the time I have spent solving technical details mostly regarding logging.\n",
    "On the other hand, at the end I have a professional experiment logging setup at https://wandb.ai/moosenet/bottlecap/overview (Send me wandb login for access if you want to see details)\n",
    "\n",
    "### Update the script for the smaller model\n",
    "I updated the hyperparametrs to the smaller models\n",
    "\n",
    "| model | dim | n_layers | n_heads | n_kv_heads | max context length | parameters | val loss | download |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 260K | 64 | 5 | 8 | 4 | 512 | 260K | 1.297 | [stories260K](https://huggingface.co/karpathy/tinyllamas/tree/main/stories260K) |\n",
    "\n",
    "### Update the code to have auxilary function: Technical problems \n",
    "Mostly classics:\n",
    "1. Transfering tensors between model and training loop for logging\n",
    "2. Again transferring tensors to from cuda device to cpu for logging\n",
    "\n",
    "### Finding the right setup of next token prediction loss\n",
    "First, I implemented a naive implementation where the baseline CrossEntropy (CE) loss was summed with a `n` CrossEntropy losses for predicting `n+1` token using the same embedding `h_{n-1}`. \n",
    "\n",
    "In other words, the baseline next token prediction `NTP` CE loss used Transformer archicture and applied CE loss for context of len $n-1$ and logits for $n$ token where $n \\in \\{1,\\ldots,\\text{MAX\\_SEQ\\_LEN}\\}$. \n",
    "\n",
    "Each of the `NiTP` Next $n + i$ token prediction loss used extra $head_i$.\n",
    "The hyperparameter `aux_losses` allowed me to experiment with multiple losses.\n",
    "However, I will present results only with 1 auxilarry loss.\n",
    "\n",
    "**It was important** to scale down the influence of the auxilarry loss so I introduced weigthing of the NiTP loss.\n",
    "\n",
    "I also introduced **consistency loss** between NTP logits for context of length $n - 1 +i$ and NiTP logits predicting $n + i$ token from context $n-1$ ie the same heads I used for NiTP loss.\n",
    "The basic idea that the transformer should predict the same distributions even for context in the future.\n",
    "I used KL divergance loss\n",
    "\n",
    "\n",
    "## Experiments notes\n",
    "\n",
    "I have run only very basic experiments due to time constraints (2h on Tuesday and 5h on Thursday).\n",
    "I submit the first reasonable result.\n",
    "\n",
    "The evaluated the speed of training I in terms of NTP (Next token prediction cross entropy) validation and training loss improvement per iteration step.\n",
    "I used the default and fixed `batch_size`, `learning_rate`, `optimizer`, ..., `max_iter` parameters.\n",
    "\n",
    "The screenshots below depicts four setups\n",
    "1. [Base (Cyan)](): a baseline where no auxilarry loss was used\n",
    "2. [NiTP (Yellow)](https://wandb.ai/moosenet/bottlecap/runs/q1jnto67/overview): On top of baseline only Next i=2 Token Prediction CE loss was used with weight 0.1\n",
    "3. [Consist (Seafoam)](https://wandb.ai/moosenet/bottlecap/runs/j1gi1ayi/overview): On top of baseline only Consistency loss for head predicting Next i=2 token was used the loss was mixed with 0.01 (other values like 0.1 are left for future work)\n",
    "4. [Consist&NiTP(red)](https://wandb.ai/moosenet/bottlecap/runs/808ar57j/overview): Both losses from above: combination of Consist and NiTP losses\n",
    "\n",
    "**On the image below you see that at 20k steps the _NiTP(yellow)_ is actually better in terms of validation NTP loss.**\n",
    "At the same time, you see that the consistency loss and setup with both consistency and NiTP loss perform worse than the baseline.\n",
    "\n",
    "However, on the second and third image below, the situation is different at 3k steps;  all three setups with auxillary losses outperformed the baseline as seen on the images bellow. \n",
    "It may suggest that auxillary loss scheduling or simple better hyperparameters setup could show that consistency loss may be useful.\n",
    "\n",
    "![val20k.png](https://raw.githubusercontent.com/oplatek/llama2.c/refs/heads/oplatek/doc/val20k.png?token=GHSAT0AAAAAACOZMRFDEKKAIQLQP33NV5WGZ7PB2RQ)\n",
    "\n",
    "\n",
    "![val3k.png](https://raw.githubusercontent.com/oplatek/llama2.c/refs/heads/oplatek/doc/val3k.png?token=GHSAT0AAAAAACOZMRFCGZOZFCDCR7AQIVOCZ7PB6OA)\n",
    "\n",
    "![train3k.png](https://raw.githubusercontent.com/oplatek/llama2.c/refs/heads/oplatek/doc/train3k.png?token=GHSAT0AAAAAACOZMRFDF42DISG7V2QOWKHSZ7PB7QA)\n",
    "\n",
    "<!-- https://github.com/oplatek/llama2.c/blob/oplatek/doc/train3k.png?raw=true) -->\n",
    "\n",
    "## Note on auxilarry losses behaviour\n",
    "I noticed interesting behaviour of the three setups with auxilary losses. \n",
    "In the image below there the same three setups:\n",
    "\n",
    "2. [NiTP (Yellow)](https://wandb.ai/moosenet/bottlecap/runs/q1jnto67/overview): On top of baseline only Next i=2 Token Prediction CE loss was used with weight 0.1\n",
    "3. [Consist (Seafoam)](https://wandb.ai/moosenet/bottlecap/runs/j1gi1ayi/overview): On top of baseline only Consistency loss for head predicting Next i=2 token was used the loss was mixed with 0.01 (other values like 0.1 are left for future work)\n",
    "4. [Consist&NiTP(red)](https://wandb.ai/moosenet/bottlecap/runs/808ar57j/overview): Both losses from above: combination of Consist and NiTP losses\n",
    "\n",
    "and we logged the `ntp` loss the `aux_loss_0` which was NiTP, Consist and NiTP loss for setups 2, 3, 4.\n",
    "The setup 4 logged also the consistency loss as the second auxillary loss `aux_loss_0`.\n",
    "\n",
    "**Interestingly the consistency loss seems to be completely minimize around 3k and then it is probably in conflict with the NTP loss because it's value rises ten times to 0.2 around 15k and then decreases consistenly till the end.**\n",
    "\n",
    "Note we logged the total loss function which was used in the optimizer but for evaluation one should looked only at NTP loss for comparison and to the individual auxilarry functions for more detailed behaviour.\n",
    "![auxilary losses](https://raw.githubusercontent.com/oplatek/llama2.c/refs/heads/oplatek/doc/aux_losses.png?token=GHSAT0AAAAAACOZMRFDCBPASTK6JUTVDHHGZ7PBZZA)\n",
    "\n",
    "## Summary\n",
    "I managed to run the training on both CPU (on Mac) and GPU.\n",
    "I achieved a prosiming prelimiary results where I showed that even within 1MD the next token prediction losses may (slightly) benefit the training speed.\n",
    "\n",
    "The immediate future work would be to find the optimal setup for the hyperparameters and also optimize the code so the speed up can be measured not only in terms of iterations but also in terms of wall time.\n",
    "For me personally, I would need to setup a environment with a GPU where identical runs will last the same amount of time.\n",
    "\n",
    "# Suggestions for other approaches to speed up training\n",
    "\n",
    "## Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach\n",
    "When I was thinking about predicting multiple tokens I immediately remember diffusion models and in particular a similar model from the paper [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171).\n",
    "I think it is worth reading the paper, but diffusion processes are anything but fast or efficient but in terms of iterations.\n",
    "This direction is not among the fist I would start with after thinking about it for longer than 2 minutes.\n",
    "\n",
    "## Redefining what the next tokens mean ðŸš€\n",
    "\n",
    "### Spelling loss\n",
    "I suggest that we would use new tokenization for the targets, namely used only a single letter tokens.\n",
    "We would simply have a copy of targets tokenized as letters and pointers where the original targets starts in the letter array.\n",
    "The auxilary head should simply predict the letters following the context.\n",
    "\n",
    "For this particular loss there are three easy to implement variants:\n",
    "1. True case\n",
    "2. Lower case\n",
    "3. Upper case\n",
    "\n",
    "Benefits are:\n",
    "- Targets are easy to prepare\n",
    "- Head thanks to very limited vocabulary just letters could be very small\n",
    "- LLM will learn dependencies between different tokenization (arguably)\n",
    "\n",
    "Note: This is from top of my head, I should do literature review before implementing it.\n",
    "\n",
    "### Use softer cross-entropy especially for next token\n",
    "I am not sure at the moment how fast can be implemented softer CE as defined in paper [Soft Alignment Objectives for Robust Adaptation of Language Generation](https://aclanthology.org/2023.acl-long.492.pdf),\n",
    "but for next token prediction it would be certainly beneficial to consider synonym tokens as valid prediction.\n",
    "It would smooth the gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
